{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency(g):\n",
    "    A     = nx.to_numpy_matrix(g)\n",
    "    A_hat = A/np.linalg.norm(A, ord=1, axis=1, keepdims=True)\n",
    "    \n",
    "    A_full = np.zeros((dim,dim))\n",
    "    n = A_hat.shape[0]\n",
    "    \n",
    "    A_full[:n,:n] = A_hat\n",
    "    \n",
    "    return A_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyGCN, self).__init__()\n",
    "        self.W_gcn = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layer)])\n",
    "        self.W_property = nn.Linear(dim, 2)\n",
    "    \n",
    "    def update(self, xs, adjacency, i):\n",
    "        hs = torch.relu(self.W_gcn[i](xs))\n",
    "        return torch.matmul(adjacency, hs)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        adjacency = torch.FloatTensor(create_adjacency(g)).to(device)\n",
    "        n = adjacency.size()[0]\n",
    "        \n",
    "        xs = torch.FloatTensor(np.eye(dim)).to(device)\n",
    "        \n",
    "        for i in range(layer):\n",
    "            xs = self.update(xs, adjacency, i)\n",
    "        \n",
    "        xs = torch.unsqueeze(torch.sum(xs,0),0)\n",
    "        z_properties = self.W_property(xs)\n",
    "        \n",
    "        return z_properties\n",
    "        \n",
    "    def __call__(self, index, train=True):\n",
    "        if index<=44:\n",
    "            t_properties = torch.LongTensor([1]).to(device)\n",
    "            G = nx.read_gpickle('pos_' + str(int(index)) + '.gpickle')\n",
    "        else:\n",
    "            G = nx.read_gpickle('neg_' + str(int(index-44)) + '.gpickle')\n",
    "            t_properties = torch.LongTensor([0]).to(device)\n",
    "            \n",
    "        z_properties = self.forward(G)\n",
    "        \n",
    "        if train:\n",
    "            loss = F.cross_entropy(z_properties, t_properties)\n",
    "            return loss\n",
    "        else:\n",
    "            zs     = torch.softmax(z_properties,1).to('cpu').data.numpy()\n",
    "            ts     = t_properties.to('cpu').data.numpy()\n",
    "            labels = np.argmax(zs)\n",
    "            \n",
    "            return labels, ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "    def train(self, dataset_train):\n",
    "        np.random.shuffle(dataset_train)\n",
    "        N = len(dataset_train)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N):\n",
    "            data_batch = dataset_train[i]\n",
    "            loss = self.model(data_batch)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tester class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset_test):\n",
    "\n",
    "        N = len(dataset_test)\n",
    "        label_list, t_list = [], []\n",
    "\n",
    "        for i in range(0, N):\n",
    "            data_batch = dataset_test[i]\n",
    "            labels, ts = self.model(data_batch, train=False)\n",
    "            label_list = np.append(label_list, labels)\n",
    "            t_list     = np.append(t_list, ts)\n",
    "        \n",
    "        auc       = accuracy_score(t_list, label_list)\n",
    "        precision = precision_score(t_list, label_list)\n",
    "        recall    = recall_score(t_list, label_list)\n",
    "        \n",
    "        return auc, precision, recall\n",
    "    \n",
    "    def result(self, epoch, time, loss, auc_dev, auc_test, precision, recall, file_name):\n",
    "        with open(file_name, 'a') as f:\n",
    "            result = map(str, [epoch, time, loss, auc_dev, auc_test, precision, recall])\n",
    "            f.write('\\t'.join(result) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim            = 171\n",
    "layer          = 2\n",
    "lr             = 1e-3\n",
    "lr_decay       = 0.75\n",
    "decay_interval = 20\n",
    "iteration      = 200\n",
    "num_examples   = 44+35\n",
    "\n",
    "(dim, layer, decay_interval, iteration, num_examples) = map(int, [dim, layer, decay_interval, iteration, num_examples])\n",
    "lr, lr_decay                            = map(float, [lr, lr_decay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2\n",
    "\n",
    "dataset = np.linspace(1,num_examples,num_examples)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "dataset_train, dataset_   = split_dataset(dataset, 0.8)\n",
    "dataset_dev, dataset_test = split_dataset(dataset_, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch \t Time(sec) \t Loss_train \t AUC_dev \t AUC_test \t Precision \t Recall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 1.0582 \t 73.5401 \t 0.6250 \t 0.6250 \t 0.0000 \t 0.0000\n",
      "1 \t 1.4922 \t 50.5432 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "2 \t 1.9056 \t 50.5998 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "3 \t 2.3349 \t 45.6404 \t 0.5000 \t 0.5000 \t 0.0000 \t 0.0000\n",
      "4 \t 2.7572 \t 50.7776 \t 0.6250 \t 0.6250 \t 0.5000 \t 0.6667\n",
      "5 \t 3.1919 \t 43.7368 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "6 \t 3.6107 \t 51.3717 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "7 \t 4.0159 \t 47.3253 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "8 \t 4.4174 \t 38.9070 \t 0.5000 \t 0.5000 \t 0.3333 \t 0.3333\n",
      "9 \t 4.8541 \t 43.6153 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "10 \t 5.2577 \t 40.7827 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "11 \t 5.6828 \t 39.1521 \t 0.5000 \t 0.5000 \t 0.3333 \t 0.3333\n",
      "12 \t 6.0820 \t 42.5631 \t 0.5000 \t 0.3750 \t 0.3333 \t 0.6667\n",
      "13 \t 6.5013 \t 36.9182 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "14 \t 6.9391 \t 33.1159 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "15 \t 7.3651 \t 35.1306 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "16 \t 7.7972 \t 43.0887 \t 0.6250 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "17 \t 8.2210 \t 35.5388 \t 0.5000 \t 0.6250 \t 0.5000 \t 0.6667\n",
      "18 \t 8.6552 \t 35.9095 \t 0.6250 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "19 \t 9.0660 \t 33.2653 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "20 \t 9.4826 \t 32.9266 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "21 \t 9.8724 \t 31.8623 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "22 \t 10.3019 \t 33.7114 \t 0.5000 \t 0.6250 \t 0.5000 \t 0.6667\n",
      "23 \t 10.7124 \t 33.6226 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "24 \t 11.1600 \t 37.9449 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "25 \t 11.6887 \t 33.3309 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "26 \t 12.1634 \t 29.7988 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "27 \t 12.6187 \t 26.3278 \t 0.6250 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "28 \t 13.0626 \t 27.7481 \t 0.3750 \t 0.3750 \t 0.3750 \t 1.0000\n",
      "29 \t 13.5274 \t 28.9872 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "30 \t 13.9719 \t 26.9044 \t 0.6250 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "31 \t 14.4653 \t 29.5310 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "32 \t 14.9416 \t 25.0443 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "33 \t 15.4090 \t 23.4330 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "34 \t 15.8753 \t 26.8821 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "35 \t 16.3372 \t 23.4298 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "36 \t 16.7729 \t 21.8438 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "37 \t 17.2015 \t 22.0046 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "38 \t 17.6187 \t 21.7161 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "39 \t 18.0405 \t 18.4190 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "40 \t 18.4511 \t 18.3492 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "41 \t 18.8585 \t 21.5324 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "42 \t 19.2812 \t 19.3090 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "43 \t 19.7030 \t 16.4197 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "44 \t 20.1426 \t 16.6811 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "45 \t 20.5897 \t 17.0777 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "46 \t 21.0146 \t 18.4359 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "47 \t 21.4592 \t 16.8769 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "48 \t 21.8748 \t 17.4585 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "49 \t 22.2992 \t 14.4616 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "50 \t 22.7175 \t 13.1227 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "51 \t 23.1477 \t 13.5332 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "52 \t 23.5851 \t 13.2368 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "53 \t 24.0021 \t 12.5607 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "54 \t 24.4295 \t 18.7896 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "55 \t 24.8453 \t 12.3334 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "56 \t 25.2579 \t 12.8453 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "57 \t 25.6628 \t 13.9232 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "58 \t 26.0566 \t 13.7824 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "59 \t 26.4740 \t 10.1338 \t 0.5000 \t 0.5000 \t 0.4286 \t 1.0000\n",
      "60 \t 26.8979 \t 10.9444 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "61 \t 27.3065 \t 10.0410 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "62 \t 27.7202 \t 9.1696 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "63 \t 28.1293 \t 9.5003 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "64 \t 28.5162 \t 9.4668 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "65 \t 28.9191 \t 14.5914 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "66 \t 29.3114 \t 9.5074 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "67 \t 29.6980 \t 8.9101 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "68 \t 30.0905 \t 7.5539 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "69 \t 30.4924 \t 8.8920 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "70 \t 30.9053 \t 7.8772 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "71 \t 31.3080 \t 7.3602 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "72 \t 31.7032 \t 7.4759 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "73 \t 32.0948 \t 6.9989 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "74 \t 32.5027 \t 7.3030 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "75 \t 32.9214 \t 6.4397 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "76 \t 33.3329 \t 8.9614 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "77 \t 33.7640 \t 13.7605 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "78 \t 34.1687 \t 5.9460 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "79 \t 34.5806 \t 5.8293 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "80 \t 35.0034 \t 5.8014 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "81 \t 35.4261 \t 6.6416 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "82 \t 35.8526 \t 5.4948 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "83 \t 36.2681 \t 5.3627 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "84 \t 36.6944 \t 5.2066 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "85 \t 37.1023 \t 5.4877 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "86 \t 37.5220 \t 9.5505 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "87 \t 37.9593 \t 5.7738 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "88 \t 38.3679 \t 5.0183 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "89 \t 38.7886 \t 5.1301 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "90 \t 39.2198 \t 5.4782 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "91 \t 39.6964 \t 5.2451 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "92 \t 40.1042 \t 5.5651 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "93 \t 40.5365 \t 4.6034 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "94 \t 40.9585 \t 4.4320 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "95 \t 41.3828 \t 4.4681 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "96 \t 41.8190 \t 4.3678 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "97 \t 42.2495 \t 4.3385 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "98 \t 42.6608 \t 4.8049 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "99 \t 43.0509 \t 4.3890 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "100 \t 43.4649 \t 4.4234 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "101 \t 43.8805 \t 4.1397 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "102 \t 44.3415 \t 4.0917 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "103 \t 44.7598 \t 3.6751 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "104 \t 45.1890 \t 3.7314 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "105 \t 45.6068 \t 3.6621 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "106 \t 46.0173 \t 3.7922 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "107 \t 46.4401 \t 3.4760 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "108 \t 46.8425 \t 3.7603 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "109 \t 47.2687 \t 3.5889 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "110 \t 47.6969 \t 28.3411 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "111 \t 48.1159 \t 5.1685 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "112 \t 48.5517 \t 3.6038 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "113 \t 49.0000 \t 3.6181 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "114 \t 49.4235 \t 3.5202 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "115 \t 49.8582 \t 3.4368 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "116 \t 50.2964 \t 3.4315 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "117 \t 50.7100 \t 3.3837 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "118 \t 51.1078 \t 3.2945 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "119 \t 51.4929 \t 3.1978 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "120 \t 51.8947 \t 3.2698 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "121 \t 52.2820 \t 3.3348 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "122 \t 52.6689 \t 3.1269 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "123 \t 53.0942 \t 3.1126 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "124 \t 53.5177 \t 3.1280 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "125 \t 53.9171 \t 3.0618 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "126 \t 54.3258 \t 3.2131 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "127 \t 54.7542 \t 2.9873 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "128 \t 55.1554 \t 3.1569 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "129 \t 55.5441 \t 2.9701 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "130 \t 55.9513 \t 2.9630 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "131 \t 56.3784 \t 2.9630 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "132 \t 56.7823 \t 2.8828 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "133 \t 57.1847 \t 3.1084 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "134 \t 57.5970 \t 2.8867 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "135 \t 57.9891 \t 3.0007 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "136 \t 58.3928 \t 2.9212 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "137 \t 58.8186 \t 2.9000 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "138 \t 59.2426 \t 3.1136 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "139 \t 59.6595 \t 2.7971 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 \t 60.0739 \t 2.6796 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "141 \t 60.4780 \t 2.6734 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "142 \t 60.8727 \t 2.6676 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "143 \t 61.2687 \t 2.6483 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "144 \t 61.7140 \t 2.6413 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "145 \t 62.1455 \t 2.6045 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "146 \t 62.5483 \t 2.7030 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "147 \t 62.9560 \t 2.6474 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "148 \t 63.3890 \t 3.0872 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "149 \t 63.7843 \t 2.6238 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "150 \t 64.1886 \t 2.5654 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "151 \t 64.5825 \t 2.5173 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "152 \t 64.9839 \t 2.4538 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "153 \t 65.4084 \t 2.4706 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "154 \t 65.8060 \t 2.3658 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "155 \t 66.2365 \t 2.4992 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "156 \t 66.6367 \t 2.3120 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "157 \t 67.0652 \t 2.4476 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "158 \t 67.4794 \t 2.3359 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "159 \t 67.8860 \t 2.6331 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "160 \t 68.3108 \t 2.3473 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "161 \t 68.7640 \t 2.7598 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "162 \t 69.1796 \t 2.4083 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "163 \t 69.5770 \t 2.3060 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "164 \t 69.9748 \t 2.2307 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "165 \t 70.3757 \t 2.2185 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "166 \t 70.8171 \t 2.3409 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "167 \t 71.2433 \t 2.1543 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "168 \t 71.6429 \t 2.2669 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "169 \t 72.0331 \t 2.4112 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "170 \t 72.4185 \t 2.2719 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "171 \t 72.8141 \t 2.1798 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "172 \t 73.2028 \t 2.2490 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "173 \t 73.6165 \t 2.1216 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "174 \t 74.0425 \t 2.1335 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "175 \t 74.4576 \t 2.1746 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "176 \t 74.8768 \t 2.3946 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "177 \t 75.2663 \t 2.2138 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "178 \t 75.6522 \t 2.4786 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "179 \t 76.0406 \t 2.0466 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "180 \t 76.4308 \t 2.0436 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "181 \t 76.8228 \t 2.0382 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "182 \t 77.2286 \t 1.9910 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "183 \t 77.6474 \t 2.0976 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "184 \t 78.0471 \t 1.9865 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "185 \t 78.5035 \t 2.0244 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "186 \t 78.8920 \t 2.0222 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "187 \t 79.2986 \t 1.9396 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "188 \t 79.7061 \t 2.0998 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "189 \t 80.1136 \t 1.9717 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "190 \t 80.5311 \t 1.9202 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "191 \t 80.9259 \t 2.0597 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "192 \t 81.3250 \t 1.8558 \t 0.5000 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "193 \t 81.7332 \t 2.0520 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "194 \t 82.1364 \t 1.9199 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "195 \t 82.5277 \t 1.9473 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "196 \t 82.9384 \t 2.1258 \t 0.5000 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "197 \t 83.3729 \t 1.9325 \t 0.3750 \t 0.5000 \t 0.4000 \t 0.6667\n",
      "198 \t 83.7814 \t 1.9044 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n",
      "199 \t 84.1840 \t 1.8998 \t 0.3750 \t 0.6250 \t 0.5000 \t 1.0000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "model   = MyGCN().to(device)\n",
    "trainer = Trainer(model)\n",
    "tester  = Tester(model)\n",
    "\n",
    "print('Training...')\n",
    "print('Epoch \\t Time(sec) \\t Loss_train \\t AUC_dev \\t AUC_test \\t Precision \\t Recall')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "setting = 'layers_' + str(layer)\n",
    "\n",
    "file_result = 'output/' + setting + '.txt'\n",
    "with open(file_result, 'w') as f:\n",
    "    f.write('Epoch \\t Time(sec) \\t Loss_train \\t AUC_dev \\t AUC_test \\t Precision_test \\t Recall_test\\n')\n",
    "\n",
    "for epoch in range(iteration):\n",
    "    if (epoch+1) % decay_interval == 0:\n",
    "        trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "    loss    = trainer.train(dataset_train)\n",
    "    auc_dev = tester.test(dataset_dev)[0]\n",
    "    auc_test, precision, recall = tester.test(dataset_test)\n",
    "    \n",
    "    lr_rate = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    end  = timeit.default_timer()\n",
    "    time = end - start\n",
    "\n",
    "    tester.result(epoch, time, loss, auc_dev, auc_test, precision, recall, file_result)\n",
    "    print('%d \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f' %(epoch, time, loss, auc_dev, auc_test, precision, recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
