{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency(g):\n",
    "    A     = nx.to_numpy_matrix(g)\n",
    "    A_hat = A/np.linalg.norm(A, ord=1, axis=1, keepdims=True)\n",
    "    \n",
    "    return A_hat\n",
    "\n",
    "def load_tensor(file_name, dtype):\n",
    "    return [dtype(d).to(device) for d in np.load(file_name + '.npy', allow_pickle=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyGCN, self).__init__()\n",
    "        self.W_gcn = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layer)])\n",
    "        self.W_property = nn.Linear(dim, 2)\n",
    "    \n",
    "    def update(self, xs, adjacency, i):\n",
    "        hs = torch.relu(self.W_gcn[i](xs))\n",
    "        return torch.matmul(adjacency, hs)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        adjacency = torch.FloatTensor(create_adjacency(g)).to(device)\n",
    "        \n",
    "        ### Create initial embedding: TODO\n",
    "        xs = torch.FloatTensor(np.eye(1000)).to(device)\n",
    "        \n",
    "        for i in range(layer):\n",
    "            xs = self.update(xs, adjacency, i)\n",
    "        \n",
    "        xs = torch.unsqueeze(torch.sum(xs,0),0)\n",
    "        z_properties = self.W_property(xs)\n",
    "        \n",
    "        return z_properties\n",
    "        \n",
    "    def __call__(self, index, train=True):\n",
    "        G = nx.read_gpickle('graph_' + str(int(index)) + '.gpickle')\n",
    "        \n",
    "        if index<=num_examples/2:\n",
    "            t_properties = torch.LongTensor([0]).to(device)\n",
    "        else:\n",
    "            t_properties = torch.LongTensor([1]).to(device)\n",
    "            \n",
    "        z_properties = self.forward(G)\n",
    "        \n",
    "        if train:\n",
    "            loss = F.cross_entropy(z_properties, t_properties)\n",
    "            return loss\n",
    "        else:\n",
    "            zs     = torch.softmax(z_properties,1).to('cpu').data.numpy()\n",
    "            ts     = t_properties.to('cpu').data.numpy()\n",
    "            labels = np.argmax(zs)\n",
    "            \n",
    "            return labels, ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "    def train(self, dataset_train):\n",
    "        np.random.shuffle(dataset_train)\n",
    "        N = len(dataset_train)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N):\n",
    "            data_batch = dataset_train[i]\n",
    "            loss = self.model(data_batch)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tester class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset_test):\n",
    "\n",
    "        N = len(dataset_test)\n",
    "        label_list, t_list = [], []\n",
    "\n",
    "        for i in range(0, N):\n",
    "            data_batch = dataset_test[i]\n",
    "            labels, ts = self.model(data_batch, train=False)\n",
    "            label_list = np.append(label_list, labels)\n",
    "            t_list     = np.append(t_list, ts)\n",
    "        \n",
    "        auc       = accuracy_score(t_list, label_list)\n",
    "        precision = precision_score(t_list, label_list)\n",
    "        recall    = recall_score(t_list, label_list)\n",
    "        \n",
    "        return auc, precision, recall\n",
    "    \n",
    "    def result(self, epoch, time, loss, auc_dev, auc_test, precision, recall, file_name):\n",
    "        with open(file_name, 'a') as f:\n",
    "            result = map(str, [epoch, time, loss, auc_dev, auc_test, precision, recall])\n",
    "            f.write('\\t'.join(result) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim            = 1000\n",
    "layer          = 2\n",
    "lr             = 1e-3\n",
    "lr_decay       = 0.75\n",
    "decay_interval = 20\n",
    "iteration      = 15\n",
    "num_examples   = 200\n",
    "\n",
    "(dim, layer, decay_interval, iteration, num_examples) = map(int, [dim, layer, decay_interval, iteration, num_examples])\n",
    "lr, lr_decay                            = map(float, [lr, lr_decay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2\n",
    "\n",
    "dataset = np.linspace(1,num_examples,num_examples)\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "dataset_train, dataset_   = split_dataset(dataset, 0.8)\n",
    "dataset_dev, dataset_test = split_dataset(dataset_, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch \t Time(sec) \t Loss_train \t AUC_dev \t AUC_test \t Precision \t Recall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 42.0321 \t 1047.9239 \t 0.4000 \t 0.4500 \t 0.0000 \t 0.0000\n",
      "1 \t 78.8621 \t 192.4038 \t 0.4000 \t 0.4500 \t 0.0000 \t 0.0000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "model   = MyGCN().to(device)\n",
    "trainer = Trainer(model)\n",
    "tester  = Tester(model)\n",
    "\n",
    "print('Training...')\n",
    "print('Epoch \\t Time(sec) \\t Loss_train \\t AUC_dev \\t AUC_test \\t Precision \\t Recall')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "setting = 'layers_' + str(layer)\n",
    "\n",
    "file_result = 'output/' + setting + '.txt'\n",
    "with open(file_result, 'w') as f:\n",
    "    f.write('Epoch \\t Time(sec) \\t Loss_train \\t AUC_dev \\t AUC_test \\t Precision_test \\t Recall_test\\n')\n",
    "\n",
    "for epoch in range(iteration):\n",
    "    if (epoch+1) % decay_interval == 0:\n",
    "        trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "    loss    = trainer.train(dataset_train)\n",
    "    auc_dev = tester.test(dataset_dev)[0]\n",
    "    auc_test, precision, recall = tester.test(dataset_test)\n",
    "    \n",
    "    lr_rate = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    end  = timeit.default_timer()\n",
    "    time = end - start\n",
    "\n",
    "    tester.result(epoch, time, loss, auc_dev, auc_test, precision, recall, file_result)\n",
    "    print('%d \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f' %(epoch, time, loss, auc_dev, auc_test, precision, recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
